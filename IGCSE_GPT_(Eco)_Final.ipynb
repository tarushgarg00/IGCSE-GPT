{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      },
      "source": [
        "# IGCSE-GPT\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7rB3szzhtx"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yw-YIET1VASa",
        "outputId": "09a0ad45-3c28-492b-a26c-099eaf589629"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "giM74oK1rRIH",
        "outputId": "f7780d36-8daa-405d-afa3-817b9ae34753"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P2Ea5_PqdkGe",
        "outputId": "209a4278-d30b-46eb-8e18-c78b72f0210f"
      },
      "outputs": [],
      "source": [
        "!pip install llmtuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RXn_YQnn9f"
      },
      "source": [
        "### Check GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      },
      "source": [
        "## Training Model\n",
        "\n",
        "It takes 20 mins for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CS0Qk5OR0i4Q",
        "outputId": "956b6311-93a3-4278-a1b6-33f83766da3a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity\",             # use alpaca and identity datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  quantization_bit=4,                     # use 4-bit QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVNaC-xS5N40"
      },
      "source": [
        "## Infer the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oh8H9A_25SF9",
        "outputId": "4eef00b1-b762-4275-81fe-4bd709caa843"
      },
      "outputs": [],
      "source": [
        "from llmtuner.chat import ChatModel\n",
        "from llmtuner.extras.misc import torch_gc\n",
        "import gradio as gr\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use clear to remove the history, use exit to exit the application.\")\n",
        "\n",
        "def AskGPT(Question):\n",
        "  query = Question\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  # print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    # print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  # print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "  return response\n",
        "\n",
        "demo = gr.Interface(\n",
        "  fn=AskGPT,\n",
        "  inputs=[gr.Textbox(label=\"Question\", lines=3, placeholder='Type here')],\n",
        "  outputs=[gr.Textbox(label=\"Answer\", lines=3)],\n",
        "  examples=[[\"Define monetary policy\"], [\"What are the macroeconomic aims of a government?\"], ['How do you calculate PED?']],\n",
        "  description='Ask your doubts about IGCSE Economics in the \\'Question\\' box',\n",
        "  title='IGCSE GPT by Aarav Patel and Tarush Garg',\n",
        "  theme='soft',\n",
        "  submit_btn='Ask'\n",
        ")\n",
        "\n",
        "demo.launch()\n",
        "\n",
        "torch_gc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
